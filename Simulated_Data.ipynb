{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear time-series generator (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def generate_random_dataset(\n",
    "    n_samples=5000,\n",
    "    n_cont_features=10,\n",
    "    n_cat_features=5,\n",
    "    n_classes=3,\n",
    "    sequence_length=None,   # now optional\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate an unbiased classification dataset using a randomly initialized neural network.\n",
    "    Automatically chooses a valid sequence_length for LSTM reshaping.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # ---- 1. Generate input features ----\n",
    "    X_cont = np.random.randn(n_samples, n_cont_features)\n",
    "    X_cat = np.random.randint(0, 5, size=(n_samples, n_cat_features))\n",
    "    X = np.hstack([X_cont, X_cat])\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # ---- 2. Random neural network to generate labels ----\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_dim, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, n_classes)\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X, dtype=torch.float32))\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "        y = np.argmax(probs, axis=1)\n",
    "\n",
    "    # ---- 3. Prepare MLP + CNN ----\n",
    "    X_mlp = X.copy()\n",
    "    X_cnn = X.reshape(n_samples, 1, -1)\n",
    "\n",
    "    # ---- 4. Auto-select a valid LSTM sequence_length ----\n",
    "    total_features = X.shape[1]\n",
    "\n",
    "    if sequence_length is None:\n",
    "        # choose the largest divider less than sqrt(total_features)\n",
    "        divs = [d for d in range(2, total_features+1) if total_features % d == 0]\n",
    "        divs.sort()\n",
    "        sequence_length = divs[0]   # pick smallest valid divider > 1\n",
    "\n",
    "    if total_features % sequence_length != 0:\n",
    "        raise ValueError(\n",
    "            f\"sequence_length={sequence_length} does not divide total_features={total_features}\"\n",
    "        )\n",
    "\n",
    "    features_per_step = total_features // sequence_length\n",
    "    X_lstm = X.reshape(n_samples, sequence_length, features_per_step)\n",
    "\n",
    "    return X_mlp, X_cnn, X_lstm, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLP --> receives the raw feature matrix X\n",
    "* CNN --> Receives the same 15 features, but arranged as a single channel: This is just a reshape, no new data is created.\n",
    "* LSTM --> Receives the same 15 features, but split into time steps: This also contains the same numbers, merely reorganized so the LSTM can process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mlp, X_cnn, X_lstm, y = generate_random_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 15)\n",
      "(5000, 1, 15)\n",
      "(5000, 3, 5)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_mlp.shape)\n",
    "print(X_cnn.shape)\n",
    "print(X_lstm.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Experiment (S1, S2 and S3 comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* S1 = MedianImpute → StandardScaler → OneHotEncoder\n",
    "* S2 = KNNImputer(k=5) → MinMaxScaler → TargetEncode\n",
    "* S3 = MICE(3 iters) → RobustScaler → Embedding Layer (this will be ignored for MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(range(10))\n",
    "cat_cols = list(range(10, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_S1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess_S2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", KNNImputer(n_neighbors=5)),\n",
    "            (\"scaler\", MinMaxScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"target\", TargetEncoder())\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "identity = FunctionTransformer(lambda x: x)\n",
    "\n",
    "preprocess_S3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", IterativeImputer(max_iter=3)),\n",
    "            (\"scaler\", RobustScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"identity\", identity)\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
