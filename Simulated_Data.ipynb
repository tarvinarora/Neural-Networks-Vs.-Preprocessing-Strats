{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear time-series generator (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medium_difficulty_dataset(\n",
    "    n_samples=5000,\n",
    "    n_cont_features=10,\n",
    "    n_cat_features=5,\n",
    "    n_classes=3,\n",
    "    lstm_sequence_length=None,  # optional\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Medium-difficulty synthetic dataset created by:\n",
    "    - Random nonlinear MLP (hidden truth function)\n",
    "    - Numeric interactions\n",
    "    - Useful + useless noise\n",
    "    - Balanced classes\n",
    "    - Suitable reshaping for MLP, CNN, LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. Generate base features\n",
    "    # ============================================================\n",
    "\n",
    "    # Continuous base features\n",
    "    X_cont = np.random.randn(n_samples, n_cont_features)\n",
    "\n",
    "    # Add nonlinear transforms â†’ adds medium complexity\n",
    "    X_nonlin = np.column_stack([\n",
    "        np.sin(X_cont[:, 0]),\n",
    "        X_cont[:, 1] * X_cont[:, 2],\n",
    "        np.tanh(X_cont[:, 3]),\n",
    "        np.exp(-X_cont[:, 4]**2),\n",
    "        (X_cont[:, 5] > 0).astype(float)\n",
    "    ])\n",
    "\n",
    "    # Noise features (useless)\n",
    "    X_noise = np.random.randn(n_samples, 10)\n",
    "\n",
    "    # Categorical\n",
    "    # Slightly imbalanced, but not extreme\n",
    "    X_cat = np.column_stack([\n",
    "        np.random.choice([0,1,2,3,4], size=n_samples, p=[0.4,0.2,0.2,0.1,0.1]),\n",
    "        np.random.choice([0,1,2,3,4], size=n_samples),\n",
    "        np.random.choice([0,1,2,3,4], size=n_samples),\n",
    "        np.random.choice([0,1,2,3,4], size=n_samples, p=[0.5,0.1,0.1,0.1,0.2]),\n",
    "        np.random.choice([0,1,2,3,4], size=n_samples)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Combine everything\n",
    "    X = np.hstack([X_cont, X_nonlin, X_noise, X_cat])\n",
    "    total_features = X.shape[1]\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 2. Hidden Random Neural Network to Generate Class Probabilities\n",
    "    # ============================================================\n",
    "\n",
    "    hidden_model = nn.Sequential(\n",
    "        nn.Linear(total_features, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, n_classes)\n",
    "    )\n",
    "\n",
    "    for p in hidden_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = hidden_model(torch.tensor(X, dtype=torch.float32))\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "\n",
    "    y = np.argmax(probs, axis=1)\n",
    "\n",
    "    # Balance classes (important)\n",
    "    # Re-sample to balance the dataset moderately\n",
    "    final_idx = []\n",
    "    for c in range(n_classes):\n",
    "        cls_idx = np.where(y == c)[0]\n",
    "        n_target = n_samples // n_classes\n",
    "        if len(cls_idx) > n_target:\n",
    "            cls_idx = np.random.choice(cls_idx, size=n_target, replace=False)\n",
    "        else:\n",
    "            cls_idx = np.random.choice(cls_idx, size=n_target, replace=True)\n",
    "        final_idx.append(cls_idx)\n",
    "\n",
    "    final_idx = np.concatenate(final_idx)\n",
    "    np.random.shuffle(final_idx)\n",
    "\n",
    "    X = X[final_idx]\n",
    "    y = y[final_idx]\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 3. Prepare MLP + CNN versions\n",
    "    # ============================================================\n",
    "\n",
    "    X_mlp = X.copy()\n",
    "    X_cnn = X.reshape(X.shape[0], 1, -1)   # (batch, channel=1, features)\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 4. Prepare LSTM Version\n",
    "    # ============================================================\n",
    "\n",
    "    if lstm_sequence_length is None:\n",
    "        # choose a divisor of total_features that gives medium sequence length (not too small)\n",
    "        divisors = [d for d in range(5, total_features+1) if total_features % d == 0]\n",
    "        divisors.sort()\n",
    "        lstm_sequence_length = divisors[len(divisors)//2]  # pick mid-level divisor\n",
    "\n",
    "    if total_features % lstm_sequence_length != 0:\n",
    "        raise ValueError(\"Chosen sequence length doesn't divide features\")\n",
    "\n",
    "    features_per_step = total_features // lstm_sequence_length\n",
    "    X_lstm = X.reshape(X.shape[0], lstm_sequence_length, features_per_step)\n",
    "\n",
    "    return X_mlp, X_cnn, X_lstm, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLP --> receives the raw feature matrix X\n",
    "* CNN --> Receives the same 15 features, but arranged as a single channel: This is just a reshape, no new data is created.\n",
    "* LSTM --> Receives the same 15 features, but split into time steps: This also contains the same numbers, merely reorganized so the LSTM can process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mlp, X_cnn, X_lstm, y = generate_medium_difficulty_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_mlp, columns=[f\"f{i}\" for i in range(X_mlp.shape[1])])\n",
    "\n",
    "# Convert categorical columns back to int\n",
    "for c in [f\"f{i}\" for i in range(25, 30)]:\n",
    "    df[c] = df[c].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24']\n",
      "Categorical cols: ['f25', 'f26', 'f27', 'f28', 'f29']\n",
      "df columns: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29']\n"
     ]
    }
   ],
   "source": [
    "num_cols = [f\"f{i}\" for i in range(25)]\n",
    "cat_cols = [f\"f{i}\" for i in range(25, 30)]\n",
    "\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "print(\"df columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols] = df[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0      float64\n",
      "f1      float64\n",
      "f2      float64\n",
      "f3      float64\n",
      "f4      float64\n",
      "f5      float64\n",
      "f6      float64\n",
      "f7      float64\n",
      "f8      float64\n",
      "f9      float64\n",
      "f10     float64\n",
      "f11     float64\n",
      "f12     float64\n",
      "f13     float64\n",
      "f14     float64\n",
      "f15     float64\n",
      "f16     float64\n",
      "f17     float64\n",
      "f18     float64\n",
      "f19     float64\n",
      "f20     float64\n",
      "f21     float64\n",
      "f22     float64\n",
      "f23     float64\n",
      "f24     float64\n",
      "f25    category\n",
      "f26    category\n",
      "f27    category\n",
      "f28    category\n",
      "f29    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_S1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "preprocess_S2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", KNNImputer(n_neighbors=5)),\n",
    "            (\"scaler\", MinMaxScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"target\", TargetEncoder())\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "identity = FunctionTransformer(lambda x: x)\n",
    "\n",
    "preprocess_S3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", IterativeImputer(max_iter=3)),\n",
    "            (\"scaler\", RobustScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"identity\", identity)\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 (4998, 50)\n",
      "S2 (4998, 30)\n",
      "S3 (4998, 30)\n"
     ]
    }
   ],
   "source": [
    "for name, preproc in [(\"S1\", preprocess_S1), (\"S2\", preprocess_S2), (\"S3\", preprocess_S3)]:\n",
    "    Xp = preproc.fit_transform(df, y)\n",
    "    print(name, Xp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 30)\n",
      "(4998, 1, 30)\n",
      "(4998, 10, 3)\n",
      "(4998,)\n"
     ]
    }
   ],
   "source": [
    "print(X_mlp.shape)\n",
    "print(X_cnn.shape)\n",
    "print(X_lstm.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.710808</td>\n",
       "      <td>0.308763</td>\n",
       "      <td>2.355629</td>\n",
       "      <td>-0.042540</td>\n",
       "      <td>0.180019</td>\n",
       "      <td>-0.310260</td>\n",
       "      <td>0.667262</td>\n",
       "      <td>0.362209</td>\n",
       "      <td>-0.676047</td>\n",
       "      <td>-0.114353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959218</td>\n",
       "      <td>-0.110982</td>\n",
       "      <td>-0.357152</td>\n",
       "      <td>1.688971</td>\n",
       "      <td>0.092737</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.290487</td>\n",
       "      <td>1.542297</td>\n",
       "      <td>1.159330</td>\n",
       "      <td>-0.103989</td>\n",
       "      <td>-0.488313</td>\n",
       "      <td>-0.609441</td>\n",
       "      <td>-2.585653</td>\n",
       "      <td>0.353820</td>\n",
       "      <td>0.780324</td>\n",
       "      <td>0.137902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059107</td>\n",
       "      <td>-2.016945</td>\n",
       "      <td>0.774187</td>\n",
       "      <td>0.088960</td>\n",
       "      <td>-1.605960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.287306</td>\n",
       "      <td>-0.189568</td>\n",
       "      <td>-0.060016</td>\n",
       "      <td>0.385205</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.133395</td>\n",
       "      <td>1.308918</td>\n",
       "      <td>0.565596</td>\n",
       "      <td>-0.958804</td>\n",
       "      <td>0.740674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557216</td>\n",
       "      <td>1.644009</td>\n",
       "      <td>1.646138</td>\n",
       "      <td>0.225626</td>\n",
       "      <td>-0.827511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.290487</td>\n",
       "      <td>1.542297</td>\n",
       "      <td>1.159330</td>\n",
       "      <td>-0.103989</td>\n",
       "      <td>-0.488313</td>\n",
       "      <td>-0.609441</td>\n",
       "      <td>-2.585653</td>\n",
       "      <td>0.353820</td>\n",
       "      <td>0.780324</td>\n",
       "      <td>0.137902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059107</td>\n",
       "      <td>-2.016945</td>\n",
       "      <td>0.774187</td>\n",
       "      <td>0.088960</td>\n",
       "      <td>-1.605960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.290487</td>\n",
       "      <td>1.542297</td>\n",
       "      <td>1.159330</td>\n",
       "      <td>-0.103989</td>\n",
       "      <td>-0.488313</td>\n",
       "      <td>-0.609441</td>\n",
       "      <td>-2.585653</td>\n",
       "      <td>0.353820</td>\n",
       "      <td>0.780324</td>\n",
       "      <td>0.137902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059107</td>\n",
       "      <td>-2.016945</td>\n",
       "      <td>0.774187</td>\n",
       "      <td>0.088960</td>\n",
       "      <td>-1.605960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0 -1.710808  0.308763  2.355629 -0.042540  0.180019 -0.310260  0.667262   \n",
       "1 -1.290487  1.542297  1.159330 -0.103989 -0.488313 -0.609441 -2.585653   \n",
       "2 -0.287306 -0.189568 -0.060016  0.385205 -0.000263 -0.133395  1.308918   \n",
       "3 -1.290487  1.542297  1.159330 -0.103989 -0.488313 -0.609441 -2.585653   \n",
       "4 -1.290487  1.542297  1.159330 -0.103989 -0.488313 -0.609441 -2.585653   \n",
       "\n",
       "         f7        f8        f9  ...       f20       f21       f22       f23  \\\n",
       "0  0.362209 -0.676047 -0.114353  ...  0.959218 -0.110982 -0.357152  1.688971   \n",
       "1  0.353820  0.780324  0.137902  ... -0.059107 -2.016945  0.774187  0.088960   \n",
       "2  0.565596 -0.958804  0.740674  ... -0.557216  1.644009  1.646138  0.225626   \n",
       "3  0.353820  0.780324  0.137902  ... -0.059107 -2.016945  0.774187  0.088960   \n",
       "4  0.353820  0.780324  0.137902  ... -0.059107 -2.016945  0.774187  0.088960   \n",
       "\n",
       "        f24  f25  f26  f27  f28  f29  \n",
       "0  0.092737  3.0  4.0  1.0  3.0  0.0  \n",
       "1 -1.605960  0.0  0.0  4.0  0.0  0.0  \n",
       "2 -0.827511  1.0  3.0  0.0  1.0  4.0  \n",
       "3 -1.605960  0.0  0.0  4.0  0.0  0.0  \n",
       "4 -1.605960  0.0  0.0  4.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_mlp, columns=[f\"f{i}\" for i in range(X_mlp.shape[1])])\n",
    "\n",
    "num_cols = [f\"f{i}\" for i in range(25)]\n",
    "cat_cols = [f\"f{i}\" for i in range(25, 30)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_S1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess_S2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", KNNImputer(n_neighbors=5)),\n",
    "            (\"scaler\", MinMaxScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"target\", TargetEncoder())\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "identity = FunctionTransformer(lambda x: x)\n",
    "\n",
    "preprocess_S3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", IterativeImputer(max_iter=3)),\n",
    "            (\"scaler\", RobustScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"identity\", identity)\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 (4998, 50)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "S2 (4998, 30)\n",
      "S3 (4998, 30)\n"
     ]
    }
   ],
   "source": [
    "for name, preproc in [(\"S1\", preprocess_S1), (\"S2\", preprocess_S2), (\"S3\", preprocess_S3)]:\n",
    "    Xp = preproc.fit_transform(df, y)\n",
    "    print(name, Xp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24']\n",
      "Categorical cols: ['f25', 'f26', 'f27', 'f28', 'f29']\n",
      "df columns: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29']\n"
     ]
    }
   ],
   "source": [
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "print(\"df columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Experiment (S1, S2 and S3 comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* S1 = MedianImpute â†’ StandardScaler â†’ OneHotEncoder\n",
    "* S2 = KNNImputer(k=5) â†’ MinMaxScaler â†’ TargetEncode\n",
    "* S3 = MICE(3 iters) â†’ RobustScaler â†’ Embedding Layer (this will be ignored for MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Utility: Convert sparse matrix â†’ dense\n",
    "# ===============================================================\n",
    "\n",
    "def to_dense(X):\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        return X.toarray()\n",
    "    return X\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MLP with Embeddings (Used for S3 Only)\n",
    "# ===============================================================\n",
    "\n",
    "class MLP_with_Embeddings(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_cardinalities, embed_dim=8, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(card, embed_dim) for card in cat_cardinalities\n",
    "        ])\n",
    "\n",
    "        # Lazy initialization for fc1\n",
    "        self.fc1 = None\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def initialize_fc1(self, input_dim):\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(input_dim, 64)\n",
    "\n",
    "    def forward(self, x_num, x_cat=None):\n",
    "\n",
    "        if x_cat is None:\n",
    "            # S1/S2\n",
    "            inp = x_num\n",
    "        else:\n",
    "            # S3 â†’ Embed categorical IDs\n",
    "            embedded = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "            embedded = torch.cat(embedded, dim=1)\n",
    "            inp = torch.cat([x_num, embedded], dim=1)\n",
    "\n",
    "        # Lazy init\n",
    "        if self.fc1 is None:\n",
    "            self.initialize_fc1(inp.shape[1])\n",
    "\n",
    "        x = torch.relu(self.fc1(inp))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Training + Evaluation\n",
    "# ===============================================================\n",
    "\n",
    "def train(model, optimizer, criterion, X_num, X_cat, y, epochs=20):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_num, X_cat)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, X_num, X_cat, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_num, X_cat)\n",
    "        preds = preds.argmax(dim=1)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        f1 = f1_score(y, preds, average=\"weighted\")\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Split numeric + categorical for S3\n",
    "# ===============================================================\n",
    "\n",
    "def split_for_strategy(X, strategy, num_numeric, num_categorical):\n",
    "    X = to_dense(X)\n",
    "\n",
    "    if strategy in [\"S1\", \"S2\"]:\n",
    "        # Fully numeric\n",
    "        return torch.tensor(X, dtype=torch.float32), None\n",
    "\n",
    "    # ---- S3: numeric + categorical IDs ----\n",
    "    X_num = X[:, :num_numeric]\n",
    "    X_cat = X[:, num_numeric:num_numeric+num_categorical]\n",
    "\n",
    "    # Convert to int and fix negative/out-of-range values\n",
    "    X_cat = X_cat.astype(int)\n",
    "    X_cat = np.clip(X_cat, 0, 4)   # because generator uses 0â€“4 categories\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X_num, dtype=torch.float32),\n",
    "        torch.tensor(X_cat, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# MLP EXPERIMENT LOOP (S1 / S2 / S3)\n",
    "# ===============================================================\n",
    "\n",
    "def run_mlp_experiment():\n",
    "\n",
    "    results = []\n",
    "\n",
    "    preprocessors = {\n",
    "        \"S1\": preprocess_S1,\n",
    "        \"S2\": preprocess_S2,\n",
    "        \"S3\": preprocess_S3\n",
    "    }\n",
    "\n",
    "    # ---- Load medium-difficulty dataset ----\n",
    "    X_mlp, _, _, y = generate_medium_difficulty_dataset()\n",
    "\n",
    "    # Determine numeric + categorical column counts dynamically\n",
    "    total_cols = X_mlp.shape[1]\n",
    "    num_categorical = 5                    # fixed by generator\n",
    "    num_numeric = total_cols - num_categorical\n",
    "\n",
    "    print(f\"\\nDetected numeric features: {num_numeric}\")\n",
    "    print(f\"Detected categorical features: {num_categorical}\")\n",
    "\n",
    "    # ---- Train/test split ----\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        X_mlp, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    for strategy, preproc in preprocessors.items():\n",
    "        print(f\"\\nðŸš€ Running MLP with {strategy}\")\n",
    "\n",
    "        # Preprocess using sklearn\n",
    "        X_train_prep = preproc.fit_transform(X_train_raw, y_train)\n",
    "        X_test_prep = preproc.transform(X_test_raw)\n",
    "\n",
    "        # Split into numeric + categorical\n",
    "        X_train_num, X_train_cat = split_for_strategy(\n",
    "            X_train_prep, strategy,\n",
    "            num_numeric=num_numeric,\n",
    "            num_categorical=num_categorical\n",
    "        )\n",
    "\n",
    "        X_test_num, X_test_cat = split_for_strategy(\n",
    "            X_test_prep, strategy,\n",
    "            num_numeric=num_numeric,\n",
    "            num_categorical=num_categorical\n",
    "        )\n",
    "\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Build model\n",
    "        model = MLP_with_Embeddings(\n",
    "            num_numeric=num_numeric,\n",
    "            cat_cardinalities=[5]*num_categorical,  # correct for generator\n",
    "            embed_dim=8,\n",
    "            num_classes=3\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train\n",
    "        train(model, optimizer, criterion,\n",
    "              X_train_num, X_train_cat, y_train_tensor)\n",
    "\n",
    "        # Evaluate\n",
    "        acc, f1 = evaluate(model,\n",
    "                           X_test_num, X_test_cat, y_test_tensor)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Model\": \"MLP\",\n",
    "            \"Preprocessing\": strategy,\n",
    "            \"Accuracy\": acc,\n",
    "            \"F1 Score\": f1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected numeric features: 25\n",
      "Detected categorical features: 5\n",
      "\n",
      "ðŸš€ Running MLP with S1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for dataframes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_indexing.py:341\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 341\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_mlp_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_mlp_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m results_mlp_df\n",
      "Cell \u001b[0;32mIn[15], line 144\u001b[0m, in \u001b[0;36mrun_mlp_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Running MLP with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Preprocess using sklearn\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m X_train_prep \u001b[38;5;241m=\u001b[39m \u001b[43mpreproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m X_test_prep \u001b[38;5;241m=\u001b[39m preproc\u001b[38;5;241m.\u001b[39mtransform(X_test_raw)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Split into numeric + categorical\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:993\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m    991\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:552\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    550\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    551\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 552\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_indexing.py:343\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    341\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    347\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for dataframes."
     ]
    }
   ],
   "source": [
    "results_mlp_df = run_mlp_experiment()\n",
    "results_mlp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Experiment (S1, S2 and S3 comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_features_after_preprocessing, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input: (batch, 1, num_features)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Determine flattened size dynamically\n",
    "        self.flatten_dim = None\n",
    "\n",
    "        self.fc1 = None\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def initialize_fc1(self, x):\n",
    "        # Called once to initialize the fully connected layer\n",
    "        if self.fc1 is None:\n",
    "            self.flatten_dim = x.shape[1]\n",
    "            self.fc1 = nn.Linear(self.flatten_dim, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, features)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Lazy initialization of the first FC layer\n",
    "        if self.fc1 is None:\n",
    "            self.initialize_fc1(x)\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, optimizer, criterion, X, y, epochs=20):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cnn(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X)\n",
    "        preds = preds.argmax(dim=1)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        f1 = f1_score(y, preds, average=\"weighted\")\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_cnn(X):\n",
    "    # after preprocessing X is (batch, features)\n",
    "    # CNN expects: (batch, 1, features)\n",
    "    return torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn = []\n",
    "\n",
    "# Use the original tabular version X_mlp for preprocessing, not X_cnn\n",
    "X_mlp, X_cnn_raw, X_lstm_raw, y = generate_random_dataset()\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_mlp, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "for strategy, preproc in preprocessors.items():\n",
    "    print(f\"\\nðŸš€ Running CNN with {strategy}\")\n",
    "\n",
    "    # 1. Preprocess tabular input\n",
    "    X_train_prep = preproc.fit_transform(X_train_raw, y_train)\n",
    "    X_test_prep = preproc.transform(X_test_raw)\n",
    "\n",
    "    # 2. Convert to CNN format: (batch, 1, features)\n",
    "    X_train_tensor = reshape_for_cnn(X_train_prep)\n",
    "    X_test_tensor = reshape_for_cnn(X_test_prep)\n",
    "\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # 3. Build CNN model\n",
    "    num_features = X_train_prep.shape[1]\n",
    "    model = SimpleCNN(num_features_after_preprocessing=num_features, num_classes=3)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 4. Train\n",
    "    train_cnn(model, optimizer, criterion, X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # 5. Evaluate\n",
    "    acc, f1 = evaluate_cnn(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # 6. Store results\n",
    "    results_cnn.append({\n",
    "        \"Model\": \"CNN\",\n",
    "        \"Preprocessing\": strategy,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "\n",
    "results_cnn_df = pd.DataFrame(results_cnn)\n",
    "results_cnn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
